{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building sentiment classification using word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\maninaya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('https://www.dropbox.com/s/8yq0edd4q908xqw/airline_sentiment.csv?dl=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sample of the dataset looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1  @VirginAmerica plus you've added commercials t...\n",
       "1                  0  @VirginAmerica it's really aggressive to blast...\n",
       "2                  0  @VirginAmerica and it's a really big bad thing...\n",
       "3                  0  @VirginAmerica seriously would pay $30 a fligh...\n",
       "4                  1  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the input text\n",
    "* Preprocess the input sentences to remove punctuation\n",
    "* Lowercasing for all words.\n",
    "* Remove the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[^0-9a-zA-Z]+',' ',text)\n",
    "    words = text.split()\n",
    "    words2 = [word for word in words if word not in stop]\n",
    "    words3=' '.join(words2)\n",
    "    return(words3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After Preprocessing the Dataset looks as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica plus added commercials experienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>virginamerica seriously would pay 30 flight se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airline_sentiment                                               text\n",
       "0                  1  virginamerica plus added commercials experienc...\n",
       "1                  0  virginamerica really aggressive blast obnoxiou...\n",
       "2                  0                 virginamerica really big bad thing\n",
       "3                  0  virginamerica seriously would pay 30 flight se...\n",
       "4                  1  virginamerica yes nearly every time fly vx ear..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the input text into a list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_words=[]\n",
    "for i in range(len(data)):\n",
    "    list_words.append(data['text'][i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a CBOW model, where the context window size is 5 and the vector length is 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maninaya\\Anaconda3\\envs\\maninaya\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(size=100, window=5, min_count=30, sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the vocabulary to model and then train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6809487, 12585800)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.build_vocab(list_words)\n",
    "model.train(list_words, total_examples=model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the average vector of a given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maninaya\\Anaconda3\\envs\\maninaya\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "features= []\n",
    "for i in range(len(list_words)):\n",
    "    t2 = list_words[i]\n",
    "    z = np.zeros((1,100))\n",
    "    k=0\n",
    "    for j in range(len(t2)):\n",
    "        try:\n",
    "            z = z+model[t2[j]]\n",
    "            k= k+1\n",
    "        except KeyError:\n",
    "            continue\n",
    "    features.append(z/k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print first element of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9752347  -0.27016597  0.70575761  2.02127039  0.23110319 -0.58973696\n",
      "  -1.58623463  0.76974845  0.24209128  1.18256747  0.10017001  0.9132099\n",
      "   0.42699955 -1.32334952 -0.20311717  0.91257919 -1.30829659  0.24698428\n",
      "   1.20008349 -0.63479377 -0.5038844   0.40085929  0.71319471 -0.30997811\n",
      "  -1.19385314  0.69267646 -0.69100454 -0.28000922  0.36959147  1.18292203\n",
      "  -1.13867408 -0.71333447  1.06983713  0.09232059 -0.27037012  0.77410247\n",
      "  -0.7799902  -0.42059729  0.51543218 -0.3691845   0.29262251 -0.04271292\n",
      "  -0.53452698  0.78014304  0.28128824  0.79487153  0.51127694  1.59902956\n",
      "  -0.80974678  0.95499197  0.44596243 -0.63527008 -0.31866452  0.28614587\n",
      "   0.36727148 -0.12961707  0.8757535  -0.89195958 -0.24674536 -0.48642722\n",
      "  -0.67119573 -1.17169897 -0.35880532  0.6722302  -1.64867594 -0.07470235\n",
      "  -0.35535427  1.0077929  -1.19501923 -0.03629139 -0.44627865  0.07526087\n",
      "  -0.52079657  0.03860507 -0.65691359 -0.79518194  0.16919739 -1.75692097\n",
      "   0.94030108  0.45052017  0.99923423 -0.15399582  0.27890911 -0.01953133\n",
      "  -1.12534589 -0.9893669  -0.35319051 -0.79211112 -0.16211635  1.19233618\n",
      "  -0.67229009  0.96926528 -0.06565299  0.98387635  0.45786782  0.16280641\n",
      "   1.153397   -1.05830386 -0.59002388 -0.56444899]]\n"
     ]
    }
   ],
   "source": [
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are taking the average of the word vectors for all the words present in the input sentence. Additionally, there will be certain words that are not in the vocabulary (words that occur less frequently) and would result in an error if we try to extract their vectors. We've deployed try and catch errors for this specific scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess features to convert them into an array, split the dataset, into train and test datasets and reshape the datasets so that they can be passed to model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array(features)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data['airline_sentiment'], test_size=0.30,random_state=10)\n",
    "X_train = X_train.reshape(X_train.shape[0],100)\n",
    "X_test = X_test.reshape(X_test.shape[0],100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print first element of X_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.64189334 -0.08338755 -0.8285354   0.82467048 -1.02868349 -0.70144425\n",
      " -0.15247449  1.08180206  0.41147404  0.4075205  -0.21401518 -0.27407975\n",
      " -0.11124514 -0.12706222  0.9429029   0.24867104 -0.92390297  0.33188284\n",
      " -0.21333201 -0.98242351  0.86084474  0.36444499  0.24323412 -0.08319756\n",
      " -0.09810477  0.62146915  0.94574724  0.77698617  0.41386133  0.27470819\n",
      " -0.09095638  0.52962661  0.66286149 -0.75652258  0.28187069  0.37262182\n",
      " -0.05396645  0.76291973 -0.38055352 -0.29811106 -0.88408418  0.35670871\n",
      " -0.25927967  0.36336555  0.04383947  0.21175945  0.70749708 -0.33536115\n",
      " -0.14764456  0.39910796 -0.02710901 -0.41864921 -0.81636152 -0.62623763\n",
      "  0.0469867  -0.11417671  0.04190523  0.05552077  0.24671666  0.98658287\n",
      "  0.69590224 -0.78081633 -0.29835591  1.0138798  -1.55551763  0.02205896\n",
      " -0.62944871  0.63498874 -0.36552272  0.15407907  0.41989354  0.7547316\n",
      " -0.27643165 -0.465283   -0.41696574  0.01783279 -0.17925515 -0.71615679\n",
      "  0.39568957 -0.75882583  0.38768501  0.93064707  1.00375148  0.19236991\n",
      "  0.39125973  0.63772872  0.76662227  0.29368225  0.46406994  0.09084332\n",
      "  0.09364767  0.62270919  0.42692022  0.31017223  1.03858729 -0.16071181\n",
      "  0.16092969  1.05495288 -0.31781097 -0.10834087]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print first element of y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9091     0\n",
      "3846     1\n",
      "4265     1\n",
      "6918     0\n",
      "7624     0\n",
      "9641     0\n",
      "2548     1\n",
      "4059     0\n",
      "3669     0\n",
      "9199     0\n",
      "7566     0\n",
      "817      0\n",
      "3688     0\n",
      "5884     0\n",
      "3851     0\n",
      "1258     0\n",
      "6793     0\n",
      "7464     0\n",
      "1146     0\n",
      "1775     0\n",
      "2570     0\n",
      "9062     0\n",
      "8170     0\n",
      "1651     0\n",
      "3651     1\n",
      "21       0\n",
      "4734     0\n",
      "1863     0\n",
      "7995     0\n",
      "4797     1\n",
      "        ..\n",
      "3932     0\n",
      "653      0\n",
      "1406     0\n",
      "409      0\n",
      "6899     0\n",
      "11318    0\n",
      "9166     1\n",
      "8036     0\n",
      "574      0\n",
      "7290     0\n",
      "3416     0\n",
      "2102     0\n",
      "2443     0\n",
      "239      0\n",
      "4452     0\n",
      "5648     0\n",
      "10742    0\n",
      "6400     0\n",
      "9289     1\n",
      "9224     0\n",
      "10234    0\n",
      "10141    1\n",
      "1520     0\n",
      "4829     1\n",
      "10201    1\n",
      "9372     0\n",
      "7291     0\n",
      "1344     0\n",
      "7293     0\n",
      "1289     0\n",
      "Name: airline_sentiment, Length: 8078, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and build the neural network to predict the sentiment of a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maninaya\\Anaconda3\\envs\\maninaya\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1000)              101000    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 1001      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 102,001\n",
      "Trainable params: 102,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1000,input_dim = 100,activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the preceding model, we have a 1,000-dimensional hidden layer that connects the 100 inputted average word vector values to the output, which has a value of 1 (1 or 0 for a positive or negative sentiment, respectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\maninaya\\Anaconda3\\envs\\maninaya\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maninaya\\Anaconda3\\envs\\maninaya\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8078 samples, validate on 3463 samples\n",
      "Epoch 1/5\n",
      "8078/8078 [==============================] - 1s 181us/step - loss: 0.3114 - acc: 0.8689 - val_loss: 0.2662 - val_acc: 0.8891\n",
      "Epoch 2/5\n",
      "8078/8078 [==============================] - 0s 44us/step - loss: 0.2517 - acc: 0.8996 - val_loss: 0.2566 - val_acc: 0.8908\n",
      "Epoch 3/5\n",
      "8078/8078 [==============================] - 0s 60us/step - loss: 0.2384 - acc: 0.9054 - val_loss: 0.2724 - val_acc: 0.8854\n",
      "Epoch 4/5\n",
      "8078/8078 [==============================] - 0s 52us/step - loss: 0.2258 - acc: 0.9091 - val_loss: 0.2522 - val_acc: 0.8955\n",
      "Epoch 5/5\n",
      "8078/8078 [==============================] - 0s 58us/step - loss: 0.2120 - acc: 0.9172 - val_loss: 0.2511 - val_acc: 0.8946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a71668bef0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=5, validation_data=(X_test, y_test),verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy of our model is ~90% in predicting the sentiment of a tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2639,  130],\n",
       "       [ 235,  459]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred2 = np.where(pred>0.5,1,0)\n",
    "confusion_matrix(y_test, pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above, we see that in 2,639 sentences, we predicted them to be positive and they are actually positive. 130 sentences were predicted to be negative and happened to be positive. 235 sentences were predicted to be positive and happened to be negative and finally, 459 sentences were predicted negative and were actually negative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
